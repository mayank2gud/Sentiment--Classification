{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/kawinm/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import os\n",
    "import spacy\n",
    "from torchtext.vocab import GloVe, FastText\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "\n",
    "pos_set = []\n",
    "neg_set = []\n",
    "with open(\"./Train dataset.csv\", encoding='utf-8') as csvf:\n",
    "    data = csv.DictReader(csvf)\n",
    "\n",
    "    for rows in data:\n",
    "\n",
    "        # Removing punctuations\n",
    "        chars_to_remove = ['¡', '§', '…','‘', '’', '¿', '«', '»', '¨', '%', '-', '“', '”', '--', '`', '~', '<', '>', '*', '{', '}', '^', '=', '_', '[', ']', '|', '- ', '/', '<br />']\n",
    "        \n",
    "        review = rows['review'].replace('<br />', \" \", -1)\n",
    "        for char in chars_to_remove:\n",
    "            review = review.replace(char, \" \", -1)\n",
    "\n",
    "        if rows['sentiment'] == 'positive':\n",
    "            pos_set.append(review)\n",
    "        else:\n",
    "            neg_set.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed = 42):\n",
    "    '''\n",
    "        For Reproducibility: Sets the seed of the entire notebook.\n",
    "    '''\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Sets a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Positive Finished ---\n",
      "--- Negative Finished ---\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# Downloads GloVe and FastText\n",
    "global_vectors = GloVe(name='840B', dim=300)\n",
    "\n",
    "# ----------- Text Preprocessing -----------\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "data_set = []\n",
    "vocab = []\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "for line in pos_set:\n",
    "\n",
    "    # Tokenizes the input text into words\n",
    "    tokens = tokenizer(line)\n",
    "\n",
    "    data_set.append((tokens, 1))\n",
    "    # Adds the extracted words to a list\n",
    "    vocab.extend(tokens)\n",
    "\n",
    "\n",
    "print(\"--- Positive Finished ---\")\n",
    "\n",
    "for line in neg_set:\n",
    "\n",
    "    # Tokenizes the input text into words\n",
    "    tokens = tokenizer(line)\n",
    "\n",
    "    data_set.append((tokens, 0))\n",
    "    # Adds the extracted words to a list\n",
    "    vocab.extend(tokens)\n",
    "\n",
    "print(\"--- Negative Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the samples based on their sequence length\n",
    "def sort_key(s):\n",
    "    return len(s[0])\n",
    "    \n",
    "#data_set = sorted(data_set, key=sort_key)   # Sorting did not gave better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the vocabulary:  94278\n"
     ]
    }
   ],
   "source": [
    "# Stores all the unique words in the dataset and their frequencies\n",
    "vocabulary = {}\n",
    "\n",
    "# Calculates the frequency of each unique word in the vocabulary\n",
    "for word in vocab:\n",
    "    if word in vocabulary:\n",
    "        vocabulary[word] += 1\n",
    "    else:\n",
    "        vocabulary[word] = 1\n",
    "\n",
    "print(\"Number of unique words in the vocabulary: \", len(vocabulary))\n",
    "\n",
    "# Stores the integer token for each unique word in the vocabulary\n",
    "ids_vocab = {}\n",
    "\n",
    "id = 0\n",
    "\n",
    "# Assigns words in the vocabulary to integer tokens\n",
    "for word, v in vocabulary.items():\n",
    "    ids_vocab[word] = id\n",
    "    id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize(corpus, ids_vocab):\n",
    "    \"\"\"\n",
    "        Converts words in the dataset to integer tokens\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_corpus = []\n",
    "    for line, sentiment in corpus:\n",
    "        new_line = []\n",
    "        for i, word in enumerate(line):\n",
    "            if word in ids_vocab and (i == 0 or word != line[i-1]):\n",
    "                new_line.append(ids_vocab[word])\n",
    "\n",
    "        new_line = torch.Tensor(new_line).long()\n",
    "        tokenized_corpus.append((new_line, sentiment))\n",
    "\n",
    "    return tokenized_corpus\n",
    "\n",
    "token_corpus = tokenize(data_set, ids_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the embedding matrix\n",
    "emb_dim = 300\n",
    "\n",
    "embeds = torch.zeros(len(ids_vocab) + 1, emb_dim)\n",
    "\n",
    "for token, idx in ids_vocab.items():\n",
    "    embeds[idx] = global_vectors[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Valid split of 90-10\n",
    "def split_indices(n, val_pct):\n",
    "\n",
    "    # Determine size of Validation set\n",
    "    n_val = int(val_pct * n)\n",
    "\n",
    "    # Create random permutation of 0 to n-1\n",
    "    idxs = np.random.permutation(n)\n",
    "    return np.sort(idxs[n_val:]), np.sort(idxs[:n_val])\n",
    "\n",
    "train_pos_indices, val_pos_indices = split_indices(len(pos_set), 0.1)\n",
    "train_neg_indices, val_neg_indices = split_indices(len(neg_set), 0.1)\n",
    "\n",
    "train_indices = np.concatenate((train_pos_indices, train_neg_indices+len(pos_set)-1))\n",
    "val_indices = np.concatenate((val_pos_indices, val_neg_indices+len(pos_set)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# ----------- Batching the data -----------\n",
    "def collate_fn(instn):\n",
    "\n",
    "    sentence = [x[0] for x in instn]\n",
    "\n",
    "    # Pre padding\n",
    "    sen_len = [len(x[0]) for x in instn]\n",
    "    max_len = max(sen_len)\n",
    "\n",
    "    padded_sent = torch.zeros(1, max_len)\n",
    "    sentence_pad = [torch.cat((torch.zeros(max_len-len(x[0])), x[0]), dim=0) for x in instn]\n",
    "    \n",
    "    for i in sentence_pad:\n",
    "        padded_sent = torch.cat((padded_sent, i.unsqueeze(dim=0)), dim=0)\n",
    "    padded_sent = padded_sent[1:].long()\n",
    "\n",
    "    # Post padding\n",
    "    #padded_sent = pad_sequence(sentence, batch_first=True, padding_value=0)\n",
    "\n",
    "    labels = torch.Tensor([x[1] for x in instn])\n",
    "\n",
    "    return (padded_sent, labels)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_sampler   = SubsetRandomSampler(train_indices)\n",
    "train_loader    = DataLoader(token_corpus, batch_size, sampler=train_sampler, collate_fn=collate_fn)\n",
    "\n",
    "val_sampler     = SubsetRandomSampler(val_indices)\n",
    "val_loader      = DataLoader(token_corpus, batch_size, sampler=val_sampler, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Model -----------\n",
    "class BILSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeds):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeds, padding_idx=0)\n",
    "\n",
    "        self.gru = nn.GRU(input_size = 300, hidden_size = 128, num_layers = 2, batch_first = True, bidirectional = True, dropout=0.5)\n",
    "\n",
    "        self.lin1 = nn.Linear(256, 64)\n",
    "        self.lin2 = nn.Linear(64, 1)\n",
    "\n",
    "        self.lin3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, xb):\n",
    "\n",
    "        xe = self.embeddings(xb)\n",
    "        out, y = self.gru(xe)\n",
    "        \n",
    "        x = self.lin3(out).squeeze(dim=-1)\n",
    "        x = torch.softmax(x, dim=-1).unsqueeze(dim=1)\n",
    "        x = torch.bmm(x, out).squeeze(dim=1)              # Weighted average\n",
    "\n",
    "        #x = torch.cat((x, y[2][ :, :], y[3][ :, :]), dim = 1) # Tried concatenating the representation with hidden units - got similar results\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:44<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Training Loss:  0.36699220136547767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 16.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.25298161758109927\n",
      "Validation accuracy:  89.52238059514879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:42<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2  Training Loss:  0.24559462553960212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.22725237463600934\n",
      "Validation accuracy:  90.47261815453864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:43<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3  Training Loss:  0.21485655704605663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.21284557785838842\n",
      "Validation accuracy:  91.22280570142536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:41<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4  Training Loss:  0.19186829854833318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.20287661429028958\n",
      "Validation accuracy:  91.87296824206051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:45<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5  Training Loss:  0.1665186250833332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 17.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.23903465596958995\n",
      "Validation accuracy:  90.67266816704176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:45<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6  Training Loss:  0.1424109943026135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 13.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.22297459421679378\n",
      "Validation accuracy:  90.94773693423356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:43<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7  Training Loss:  0.12036320782458106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 14.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.21119020320475101\n",
      "Validation accuracy:  92.1730432608152\n",
      "Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:43<00:00,  6.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8  Training Loss:  0.09301362891437102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 16.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.25896485010161996\n",
      "Validation accuracy:  91.14778694673669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:43<00:00,  6.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9  Training Loss:  0.07259131483205244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 17.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.27155631058849394\n",
      "Validation accuracy:  91.57289322330583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 282/282 [00:42<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  10  Training Loss:  0.053940180878993785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 15.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss:  0.31143582286313176\n",
      "Validation accuracy:  91.27281820455114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "model = BILSTM(embeds)\n",
    "model.to(device)\n",
    "opt_c = torch.optim.AdamW(model.parameters(), lr = 0.001) # Same as Adam with weight decay = 0.001\n",
    "# loss_fn_c = F.cross_entropy #Tried Cross Entropy with log_softmax output function - gave similar results\n",
    "loss_fn_c = F.binary_cross_entropy\n",
    "\n",
    "# ----------- Main Training Loop -----------\n",
    "max_epoch = 10\n",
    "\n",
    "best_test_acc = 0\n",
    "for ep in range(max_epoch):\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for xb, yb in tqdm(train_loader):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        y_hat = model(xb)\n",
    "        loss = loss_fn_c(y_hat.squeeze(), yb)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        opt_c.step()\n",
    "\n",
    "        opt_c.zero_grad()\n",
    "\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "\n",
    "        epoch_loss += float(loss)\n",
    "\n",
    "    print(\"Epoch: \", ep+1, \" Training Loss: \", epoch_loss/len(train_loader))\n",
    "\n",
    "\n",
    "    #----------- Validation -----------\n",
    "\n",
    "    val_labels = []\n",
    "    val_pred = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    val_epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in tqdm(val_loader):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            y_hat = model(xb)\n",
    "            loss = loss_fn_c(y_hat.squeeze(), yb)\n",
    "\n",
    "            val_epoch_loss += float(loss)\n",
    "\n",
    "            val_labels.extend(torch.round(yb).cpu().detach().numpy())\n",
    "            val_pred.extend(y_hat.round().cpu().detach().numpy())\n",
    "\n",
    "    print(\"Validation loss: \", val_epoch_loss/len(val_loader))\n",
    "    print(\"Validation accuracy: \", accuracy_score(val_labels, val_pred)*100)\n",
    "\n",
    "    if ep > 5 and prev_val_loss - val_epoch_loss > 0.015:\n",
    "        print(\"Saving Model\")\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    \n",
    "    prev_val_loss = val_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Text Preprocessing ----------\n",
    "\n",
    "test_set = []\n",
    "with open(\"./E0334 Assignment2 Test Dataset.csv\", encoding='utf-8') as csvf:\n",
    "    data = csv.DictReader(csvf)\n",
    "\n",
    "    for rows in data:\n",
    "\n",
    "        # Removing punctuations\n",
    "        chars_to_remove = ['¡', '§', '…','‘', '’', '¿', '«', '»', '¨', '%', '-', '“', '”', '--', '`', '~', '<', '>', '*', '{', '}', '^', '=', '_', '[', ']', '|', '- ', '/', '<br />']\n",
    "        \n",
    "        review = rows['review'].replace('<br />', \" \", -1)\n",
    "        for char in chars_to_remove:\n",
    "            review = review.replace(char, \" \", -1)\n",
    "        tokens = tokenizer(review)\n",
    "\n",
    "        if rows['sentiment'] == 'positive':\n",
    "            test_set.append((tokens, 1))\n",
    "        else:\n",
    "            test_set.append((tokens, 0))\n",
    "\n",
    "test_set = sorted(test_set, key=sort_key)\n",
    "\n",
    "token_corpus_test = tokenize(test_set, ids_vocab)\n",
    "\n",
    "test_loader      = DataLoader(token_corpus_test, batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 55.92it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.2429095369916928\n",
      "Test accuracy:  92.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = BILSTM(embeds)\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.to(device)\n",
    "\n",
    "test_labels = []\n",
    "test_pred = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_epoch_loss = 0\n",
    "\n",
    "# ---------- Testing ----------\n",
    "with torch.no_grad():\n",
    "    for xb, yb in tqdm(test_loader):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "\n",
    "        y_hat = model(xb)\n",
    "        loss = loss_fn_c(y_hat.squeeze(), yb)\n",
    "\n",
    "        test_epoch_loss += float(loss)\n",
    "\n",
    "        test_labels.extend(torch.round(yb).cpu().detach().numpy())\n",
    "        test_pred.extend(y_hat.round().cpu().detach().numpy())\n",
    "\n",
    "print(\"Test loss: \", test_epoch_loss/len(test_loader))\n",
    "print(\"Test accuracy: \", accuracy_score(test_labels, test_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed doesn't work in Jupyter notebook, to replicate my results, kindly, run it as .py file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "82c36bf2dc7bc97628b9e43543d03433a2e60a09cf06bbc88105c7bffe751e99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
